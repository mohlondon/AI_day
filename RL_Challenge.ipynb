{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge \n",
    "\n",
    "to complete this challenge you have to:\n",
    "\n",
    "*   choose an environment i suggest using Acrobot the one that is already there but if you want you can choose any one you want and you will be graded according to the difficulty of the environment\n",
    "*   fill in the Todo blocks to complete the code\n",
    "* train your model, test it and for the evaluation you need to submit a video and the weights of the trained agent along with the notebook\n",
    "\n",
    "That's it Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the statment bellow to install dependencies\n",
    "# !pip install mitdeeplearning\n",
    "# !apt-get install -y xvfb python-opengl x11-utils > /dev/null 2>&1\n",
    "# !pip install gym pyvirtualdisplay scikit-video > /dev/null 2>&1\n",
    "\n",
    "# Import Tensorflow 2.0\n",
    "import tensorflow as tf\n",
    "\n",
    "import mitdeeplearning as mdl\n",
    "\n",
    "import numpy as np\n",
    "import base64, io, time, gym\n",
    "from PIL import Image\n",
    "\n",
    "import IPython, functools\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize our environment and our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating the Envirment\n",
    "ENVIRMENT_NAME = \"Acrobot-v1\"\n",
    "'''\n",
    "Todo init your enviroment here\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_observations = env.observation_space.shape\n",
    "print(\"Environment has observation space =\", n_observations)\n",
    "n_actions = env.action_space.n\n",
    "print(\"Number of possible actions that the agent can choose from =\", n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and init the agent (DQN)\n",
    "'''\n",
    "    Todo \n",
    "    propose an architecture you think will best solve the problem\n",
    "    '''\n",
    "def create_agent_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        \n",
    "        \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "challengeModel = create_agent_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(model, observation, single=True):\n",
    "    observation = np.expand_dims(observation,axis=0)\n",
    "    Q_Values_Expectation = # compute the Q-values\n",
    "    action = tf.random.categorical(Q_Values_Expectation, num_samples=1).numpy().flatten()\n",
    "    return action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our agent's memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self): \n",
    "        self.clear()\n",
    "    def clear(self): \n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "    def add_to_memory(self, new_observation, new_action, new_reward): \n",
    "        self.observations.append(new_observation)\n",
    "        self.actions.append(new_action)\n",
    "        self.rewards.append(new_reward) \n",
    "\n",
    "memory = Memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a reward and loss  functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(z):\n",
    "    z -= np.mean(z)\n",
    "    x = z / np.std(z)\n",
    "    return x.astype(np.float32)\n",
    "\n",
    "\n",
    "def discount_rewards(rewards, gamma=0.80):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    R = 0\n",
    "    for t in reversed(range(0, len(rewards))):\n",
    "        '''\n",
    "        Todo Calculate total discounted reward\n",
    "        '''\n",
    "    return normalize(discounted_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, actions, rewards):\n",
    "    neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=actions)\n",
    "    loss = tf.reduce_mean( neg_logprob * rewards ) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the learning algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo fill in the appropriate hyperparameter \n",
    "learning_rate = \n",
    "DISCOUNT_FACTOR = \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "reward_history = mdl.util.LossHistory(smoothing_factor=0.9)\n",
    "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Episodes', ylabel='Rewards')\n",
    "NUM_EPISODES = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, observations, actions, discounted_rewards):\n",
    "    with tf.GradientTape() as tape:\n",
    "        Expected_returns = model(observations)\n",
    "        loss = compute_loss(Expected_returns, actions, discounted_rewards)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Todo define the training process for your agent \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "while True:\n",
    "        action = choose_action(challengeModel, observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# record a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper function to store a video of the agent playing the game\n",
    "def save_video_of_model(model, env_name, suffix=\"\"):\n",
    "    import skvideo.io\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()\n",
    "\n",
    "    env = gym.make(env_name)\n",
    "    obs = env.reset()\n",
    "    prev_obs = obs\n",
    "\n",
    "    filename = env_name + suffix + \".mp4\"\n",
    "    output_video = skvideo.io.FFmpegWriter(filename)\n",
    "\n",
    "    counter = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        output_video.writeFrame(frame)\n",
    "        input_obs = obs\n",
    "        action = model(np.expand_dims(input_obs, 0)).numpy().argmax()\n",
    "\n",
    "        prev_obs = obs\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        counter += 1\n",
    "\n",
    "    output_video.close()\n",
    "    env.close()\n",
    "    print(\"Successfully saved {} frames into {}!\".format(counter, filename))\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 71 frames into Acrobot-v1.mp4!\n"
     ]
    }
   ],
   "source": [
    "saved_cartpole = save_video_of_model(challengeModel, ENVIRMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store the trained agent for evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "challengeModel.save_weights(f\"{ENVIRMENT_NAME}.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "uoJsVjtCMunI"
   ],
   "name": "Part2_Music_Generation_Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
