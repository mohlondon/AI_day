{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the statment bellow to install dependencies\n",
    "# !pip install mitdeeplearning\n",
    "# !apt-get install -y xvfb python-opengl x11-utils > /dev/null 2>&1\n",
    "# !pip install gym pyvirtualdisplay scikit-video > /dev/null 2>&1\n",
    "\n",
    "# Import Tensorflow 2.0\n",
    "import tensorflow as tf\n",
    "\n",
    "import mitdeeplearning as mdl\n",
    "\n",
    "import numpy as np\n",
    "import base64, io, time, gym\n",
    "from PIL import Image\n",
    "\n",
    "import IPython, functools\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import HTML\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive in, let's take a step back and outline our approach, which is generally applicable to reinforcement learning problems in general:\n",
    "\n",
    "1. **Initialize our environment and our agent**.\n",
    "2. **Define our agent's memory**.\n",
    "3. **Define a reward and loss functions**.\n",
    "4. **Define the learning algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize our environment and our agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Cartpole, a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pole starts upright, and the goal is to prevent it from falling over. The system is controlled by applying a force of +1 or -1 to the cart. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center of the track. A visual summary of the cartpole environment is depicted below:\n",
    "\n",
    "<img width=\"400px\" src=\"https://danielpiedrahita.files.wordpress.com/2017/02/cart-pole.png\"></img>\n",
    "\n",
    "Given this setup for the environment and the objective of the game, we can think about: 1) what observations help define the environment's state; 2) what actions the agent can take. \n",
    "\n",
    "First, let's consider the observation space. In this Cartpole environment our observations are:\n",
    "\n",
    "1. Cart position\n",
    "2. Cart velocity\n",
    "3. Pole angle\n",
    "4. Pole rotation rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating the Envirment\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment has observation space = (4,)\n",
      "Number of possible actions that the agent can choose from = 2\n"
     ]
    }
   ],
   "source": [
    "n_observations = env.observation_space\n",
    "print(\"Environment has observation space =\", n_observations.shape)\n",
    "n_actions = env.action_space.n\n",
    "print(\"Number of possible actions that the agent can choose from =\", n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and init the agent (DQN)\n",
    "def create_agent_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "      tf.keras.layers.Dense(units=n_actions, activation=None) ])\n",
    "    return model\n",
    "\n",
    "cartpole_model = create_agent_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(model, observation, single=True):\n",
    "    # we need to expand the observation before feeding it to the model \n",
    "    observation = np.expand_dims(observation, axis=0)\n",
    "    # feed the obs to the model and get the expected rewards\n",
    "    Q_Values_Expectation = model.predict(observation)\n",
    "    # this statment will choose the action of the largest return \n",
    "    action = tf.random.categorical(Q_Values_Expectation, num_samples=1).numpy().flatten()\n",
    "    return action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our agent's memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self): \n",
    "        self.clear()\n",
    "    def clear(self): \n",
    "        self.observations = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "    def add_to_memory(self, new_observation, new_action, new_reward): \n",
    "        self.observations.append(new_observation)\n",
    "        self.actions.append(new_action)\n",
    "        self.rewards.append(new_reward) \n",
    "\n",
    "memory = Memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a reward and loss  functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "normlize is function that calculate x according to this equation \n",
    "X = (z - mean(z))/std(z)\n",
    "\n",
    "'''\n",
    "def normalize(z):\n",
    "    z -= np.mean(z)\n",
    "    x = z / np.std(z)\n",
    "    return x.astype(np.float32)\n",
    "\n",
    "\n",
    "def discount_rewards(rewards, gamma=0.95):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    R = 0\n",
    "    for t in reversed(range(0, len(rewards))):\n",
    "        R = R * (gamma**t) + rewards[t]\n",
    "        discounted_rewards[t] = R\n",
    "\n",
    "    return normalize(discounted_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, actions, rewards):\n",
    "    neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=actions)\n",
    "    '''\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits:\n",
    "    \n",
    "    Measures the probability error in discrete \n",
    "    classification tasks in which the classes \n",
    "    are mutually exclusive (each entry is in exactly one class).\n",
    "    '''\n",
    "    loss = tf.reduce_mean( neg_logprob * rewards ) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the learning algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "reward_history = mdl.util.LossHistory(smoothing_factor=0.9)\n",
    "# to plot the total reward of each episode\n",
    "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Episodes', ylabel='Rewards')\n",
    "NUM_EPISODES = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, observations, actions, discounted_rewards):\n",
    "    with tf.GradientTape() as tape:\n",
    "        Expected_returns = model(observations)\n",
    "        loss = compute_loss(Expected_returns, actions, discounted_rewards)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(NUM_EPISODES):\n",
    "    # to plot in real time\n",
    "    plotter.plot(reward_history.get())\n",
    "    # get the first observation \n",
    "    observation = env.reset()\n",
    "    memory.clear()\n",
    "    while True:\n",
    "        # Chose an action\n",
    "        action = choose_action(cartpole_model, observation)\n",
    "        # Perform the action and get the rewards and the next state\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "        # Save the data to memory\n",
    "        memory.add_to_memory(observation, action, reward)\n",
    "        # just to visualize What's happening \n",
    "        env.render()\n",
    "        # if the episode ends train the network\n",
    "        if done:\n",
    "            # Calculate the Total Reward for the Current episode\n",
    "            total_reward = sum(memory.rewards)\n",
    "            # add the current reward to the plot\n",
    "            reward_history.append(total_reward)\n",
    "            # perform a training step over the data of the current episode \n",
    "            train_step(cartpole_model, optimizer,\n",
    "                       observations=np.vstack(memory.observations),\n",
    "                       actions=np.array(memory.actions),\n",
    "                       discounted_rewards = discount_rewards(memory.rewards))\n",
    "            # clear memory for the next episode\n",
    "            memory.clear()\n",
    "            break\n",
    "        # save the next observation to use it in the next iteration\n",
    "        observation = next_observation\n",
    "# force close the envirment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "while True:\n",
    "        action = choose_action(cartpole_model, observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "That's it! Congratulations on training RL agent and putting it to the test! I encourage you to consider the following:\n",
    "\n",
    "*   How does the agent perform ?\n",
    "*   Could you train it for shorter amounts of time  and still perform well?\n",
    "* What are some things you could change about the agent or the learning process to potentially improve performance?\n",
    "\n",
    "That's it Good luck!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "uoJsVjtCMunI"
   ],
   "name": "Part2_Music_Generation_Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
